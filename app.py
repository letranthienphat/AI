import streamlit as st
import time
import psutil
import json
import random
from datetime import datetime
from openai import OpenAI
import google.generativeai as genai
import streamlit.components.v1 as components

# --- 1. C·∫§U H√åNH H·ªÜ TH·ªêNG ---
st.set_page_config(page_title="NEXUS V65.0 - SMOOTH", layout="wide", page_icon="üß¨")

# L·∫•y Keys t·ª´ Secrets
GROQ_KEYS = st.secrets.get("GROQ_KEYS", [])
GEMINI_KEY = st.secrets.get("GEMINI_KEY", "")

# Kh·ªüi t·∫°o Session State
if 'chat_log' not in st.session_state: st.session_state.chat_log = []
if 'session_id' not in st.session_state: st.session_state.session_id = f"SES_{int(time.time())}"
if 'auto_scroll' not in st.session_state: st.session_state.auto_scroll = True

# --- 2. JAVASCRIPT: T·ª∞ ƒê·ªòNG CU·ªòN THEO T·ªêC ƒê·ªò ƒê·ªåC ---
def inject_auto_scroll():
    # JavaScript n√†y t√¨m container ch·ª©a chat v√† cu·ªôn d·∫ßn d·∫ßn
    components.html(
        """
        <script>
        var scrollInterval;
        function startAutoScroll() {
            scrollInterval = setInterval(function() {
                window.scrollTo({
                    top: document.body.scrollHeight,
                    behavior: 'smooth'
                });
            }, 500); // M·ªói 0.5 gi√¢y ki·ªÉm tra v√† cu·ªôn nh·∫π
        }
        startAutoScroll();
        </script>
        """,
        height=0,
    )

# --- 3. GIAO DI·ªÜN CYBER TERMINAL (CSS FIX NH·∫¢Y KHUNG) ---
st.markdown(f"""
    <style>
    /* Ch·ªëng nh·∫£y khung chat khi AI ƒëang stream */
    .stChatFloatingInputContainer {{
        background-color: rgba(10, 15, 20, 0.95) !important;
        border-top: 1px solid #00f2ff55 !important;
        padding-bottom: 20px !important;
    }}
    
    .stApp {{
        background: #05070a;
        color: #e0faff;
    }}

    /* Khung chat c·ªë ƒë·ªãnh */
    [data-testid="stChatMessage"] {{
        background: rgba(15, 25, 35, 0.8) !important;
        border-radius: 8px !important;
        border: 1px solid #1e293b !important;
        margin-bottom: 1rem;
    }}

    /* Widget th·ªùi gian th·ª±c */
    .stat-box {{
        padding: 15px;
        background: rgba(0, 242, 255, 0.05);
        border-left: 3px solid #00f2ff;
        border-radius: 4px;
        margin-bottom: 10px;
    }}
    </style>
    """, unsafe_allow_html=True)

# --- 4. H√ÄM X·ª¨ L√ù D·ªÆ LI·ªÜU & AI ---
def get_hardware_status():
    return {
        "cpu": psutil.cpu_percent(interval=None),
        "ram": psutil.virtual_memory().percent,
        "time": datetime.now().strftime("%H:%M:%S")
    }

def call_ai_engine(prompt, model_choice):
    # ƒê√≥ng g√≥i TO√ÄN B·ªò l·ªãch s·ª≠ (Memory vƒ©nh c·ª≠u)
    full_history = [{"role": m["role"], "content": m["content"]} for m in st.session_state.chat_log]
    full_history.append({"role": "user", "content": prompt})

    # Logic Routing 1->2->3->4->Gemini
    key_pool = list(GROQ_KEYS)
    if "Groq" in model_choice:
        idx = int(model_choice.split(" ")[-1]) - 1
        key_pool.insert(0, key_pool.pop(idx))

    for i, key in enumerate(key_pool):
        try:
            client = OpenAI(api_key=key, base_url="https://api.groq.com/openai/v1")
            return client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=full_history,
                stream=True
            ), f"Groq Node {i+1}"
        except:
            continue

    # Backup Gemini
    try:
        genai.configure(api_key=GEMINI_KEY)
        model = genai.GenerativeModel('gemini-1.5-flash')
        gem_hist = [{"role": "user" if m["role"] == "user" else "model", "parts": [m["content"]]} for m in full_history[:-1]]
        chat = model.start_chat(history=gem_hist)
        return chat.send_message(prompt, stream=True), "Gemini Ultra"
    except:
        return None, None

# --- 5. GIAO DI·ªÜN CH√çNH ---
def main():
    # SIDEBAR: MONITORING TH·ªúI GIAN TH·ª∞C
    with st.sidebar:
        st.title("üí† NEXUS CORE")
        st.markdown("---")
        
        # Monitor Hardware Real-time
        stats = get_hardware_status()
        st.markdown(f"""
        <div class="stat-box">
            <b>SYSTEM MONITOR</b><br>
            CPU: <span style="color:#00f2ff">{stats['cpu']}%</span><br>
            RAM: <span style="color:#00f2ff">{stats['ram']}%</span><br>
            Update: {stats['time']}
        </div>
        """, unsafe_allow_html=True)
        
        st.divider()
        # B·ªô ƒëi·ªÅu ph·ªëi lu·ªìng
        st.subheader("ü§ñ AI Dispatcher")
        model_selection = st.selectbox("Ch·ªçn ƒë√≠ch ƒë·∫øn:", ["Auto-Route", "Groq 1", "Groq 2", "Groq 3", "Gemini"])
        
        if st.button("üî¥ PURGE MEMORY"):
            st.session_state.chat_log = []
            st.rerun()

    # MAIN INTERFACE
    st.title("üß¨ Neural Interface")
    st.caption(f"Session ID: `{st.session_state.session_id}` | Tr√≠ nh·ªõ vƒ©nh c·ª≠u ƒëang ho·∫°t ƒë·ªông.")

    # Container hi·ªÉn th·ªã chat
    chat_container = st.container()
    
    with chat_container:
        for msg in st.session_state.chat_log:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

    # X·ª≠ l√Ω nh·∫≠p li·ªáu (ƒê∆∞·ª£c ƒë·∫∑t ·ªü ƒë√°y v√† c·ªë ƒë·ªãnh)
    if prompt := st.chat_input("Nh·∫≠p l·ªánh ƒëi·ªÅu khi·ªÉn..."):
        # 1. L∆∞u tin nh·∫Øn ng∆∞·ªùi d√πng
        st.session_state.chat_log.append({"role": "user", "content": prompt})
        with chat_container:
            with st.chat_message("user"):
                st.markdown(prompt)

        # 2. Ph·∫£n h·ªìi AI
        with chat_container:
            with st.chat_message("assistant"):
                # K√≠ch ho·∫°t t·ª± ƒë·ªông cu·ªôn
                if st.session_state.auto_scroll:
                    inject_auto_scroll()
                
                res_box = st.empty()
                full_res = ""
                stream, source = call_ai_engine(prompt, model_selection)
                
                if stream:
                    for chunk in stream:
                        content = chunk.choices[0].delta.content if "Groq" in source else chunk.text
                        if content:
                            full_res += content
                            # Hi·ªÉn th·ªã m∆∞·ª£t m√† kh√¥ng l√†m nh·∫£y thanh chat
                            res_box.markdown(full_res + "‚ñà")
                    
                    res_box.markdown(full_res)
                    st.caption(f"‚ö° Lu·ªìng d·ªØ li·ªáu: {source}")
                    st.session_state.chat_log.append({"role": "assistant", "content": full_res})
                    
                    # L∆∞u v√†o b·ªô nh·ªõ gi·∫£ l·∫≠p m√°y ch·ªß (JSON)
                    with open(f"{st.session_state.session_id}.json", "w") as f:
                        json.dump(st.session_state.chat_log, f)
                else:
                    st.error("H·ªá th·ªëng m·∫•t k·∫øt n·ªëi ho√†n to√†n.")

if __name__ == "__main__":
    main()
