import streamlit as st
from openai import OpenAI
from gtts import gTTS
from io import BytesIO
from streamlit_mic_recorder import mic_recorder
import base64

# --- Cáº¤U HÃŒNH ---
st.set_page_config(page_title="AI Nexus Gen", layout="wide", page_icon="âš¡")

# API SETUP
GROQ_API_KEY = st.secrets.get("GROQ_API_KEY", "")
# Sá»­ dá»¥ng OpenAI API cho cáº£ Chat, Audio vÃ  Image (Giáº£ Ä‘á»‹nh báº¡n dÃ¹ng OpenAI hoáº·c DALL-E)
client = OpenAI(api_key=st.secrets.get("OPENAI_API_KEY", GROQ_API_KEY)) 

# --- JAVASCRIPT Äá»˜T PHÃ (Xá»­ lÃ½ Cuá»™n trang vÃ  Tá»‘c Ä‘á»™ Ä‘á»c Instant) ---
st.markdown("""
    <script>
    // 1. Tá»± Ä‘á»™ng cuá»™n xuá»‘ng dÆ°á»›i cÃ¹ng khi cÃ³ tin nháº¯n má»›i
    const observer = new MutationObserver(() => {
        const chatContainer = window.parent.document.querySelector('section.main');
        chatContainer.scrollTo({ top: chatContainer.scrollHeight, behavior: 'smooth' });
    });
    observer.observe(window.parent.document.body, { childList: true, subtree: true });

    // 2. HÃ m thay Ä‘á»•i tá»‘c Ä‘á»™ audio ngay láº­p tá»©c
    window.changeAudioSpeed = (speed) => {
        const audios = window.parent.document.querySelectorAll('audio');
        audios.forEach(audio => { audio.playbackRate = speed; });
    }
    </script>
    """, unsafe_allow_html=True)

# CSS lÃ m Ä‘áº¹p giao diá»‡n
st.markdown("""
    <style>
    .stChatMessage { border-radius: 15px; margin-bottom: 10px; border: 1px solid #eee; }
    .stChatInputContainer { position: fixed; bottom: 20px; z-index: 1000; }
    .img-gen-card { border: 2px solid #7000ff; border-radius: 15px; padding: 10px; background: #f9f0ff; }
    </style>
    """, unsafe_allow_html=True)

# --- QUáº¢N LÃ TRáº NG THÃI ---
if "messages" not in st.session_state: st.session_state.messages = []
if "voice_draft" not in st.session_state: st.session_state.voice_draft = None
if "playback_speed" not in st.session_state: st.session_state.playback_speed = 1.0

# --- CÃ”NG Cá»¤ Xá»¬ LÃ ---
def text_to_speech(text):
    tts = gTTS(text=text, lang='vi')
    fp = BytesIO()
    tts.write_to_fp(fp)
    b64 = base64.b64encode(fp.getvalue()).decode()
    return f'<audio autoplay class="voice-audio" controls style="width:100%; height:30px;"><source src="data:audio/mp3;base64,{b64}" type="audio/mp3"></audio>'

def generate_image(prompt):
    try:
        response = client.images.generate(
            model="dall-e-3", prompt=prompt, n=1, size="1024x1024"
        )
        return response.data[0].url
    except:
        return "https://via.placeholder.com/1024x1024.png?text=Loi+Tao+Anh"

# --- SIDEBAR ---
with st.sidebar:
    st.title("âš¡ Nexus Control")
    speed = st.slider("Tá»‘c Ä‘á»™ phÃ¡t (Ãp dá»¥ng tá»©c thÃ¬)", 0.5, 2.0, st.session_state.playback_speed, 0.1)
    if speed != st.session_state.playback_speed:
        st.session_state.playback_speed = speed
        st.components.v1.html(f"<script>window.changeAudioSpeed({speed})</script>", height=0)
    
    st.divider()
    mode = st.radio("Cháº¿ Ä‘á»™ pháº£n há»“i", ["ThÃ´ng minh", "Chá»‰ táº¡o áº£nh ğŸ¨"])

# --- GIAO DIá»†N CHAT ---
for i, m in enumerate(st.session_state.messages):
    with st.chat_message(m["role"]):
        st.markdown(m["content"])
        if "image_url" in m:
            st.image(m["image_url"], caption="HÃ¬nh áº£nh Ä‘Æ°á»£c táº¡o bá»Ÿi AI")
        if m["role"] == "assistant" and i == len(st.session_state.messages) - 1:
            st.markdown(text_to_speech(m["content"]), unsafe_allow_html=True)

# --- KHU Vá»°C NHáº¬P LIá»†U ---
st.markdown("<div style='height: 100px;'></div>", unsafe_allow_html=True) # Táº¡o khoáº£ng trá»‘ng cho chat input

col_mic, col_input = st.columns([1, 9])
with col_mic:
    audio_data = mic_recorder(start_prompt="ğŸ¤", stop_prompt="â¹ï¸", key='mic')

# Xá»­ lÃ½ input giá»ng nÃ³i
if audio_data and not st.session_state.voice_draft:
    with st.spinner("Äang nghe..."):
        with open("temp.wav", "wb") as f: f.write(audio_data['bytes'])
        with open("temp.wav", "rb") as af:
            transcript = client.audio.transcriptions.create(model="whisper-1", file=af)
            st.session_state.voice_draft = transcript.text
            st.rerun()

# Hiá»ƒn thá»‹ báº£n nhÃ¡p giá»ng nÃ³i Ä‘á»ƒ sá»­a
if st.session_state.voice_draft:
    with st.container():
        st.info(f"ğŸ™ï¸ NhÃ¡p: {st.session_state.voice_draft}")
        c1, c2 = st.columns(2)
        if c1.button("ğŸš€ Gá»­i ngay"):
            user_msg = st.session_state.voice_draft
            st.session_state.voice_draft = None
            # Tá»± Ä‘á»™ng nháº­n diá»‡n Ã½ Ä‘á»‹nh táº¡o áº£nh
            img_keywords = ["váº½", "táº¡o hÃ¬nh", "áº£nh", "bá»©c tranh"]
            if any(k in user_msg.lower() for k in img_keywords) or mode == "Chá»‰ táº¡o áº£nh ğŸ¨":
                with st.spinner("ğŸ¨ Äang váº½..."):
                    url = generate_image(user_msg)
                    st.session_state.messages.append({"role": "user", "content": user_msg})
                    st.session_state.messages.append({"role": "assistant", "content": "ÄÃ¢y lÃ  tÃ¡c pháº©m cá»§a báº¡n:", "image_url": url})
            else:
                st.session_state.messages.append({"role": "user", "content": user_msg})
                res = client.chat.completions.create(
                    model="gpt-4o", # Hoáº·c model báº¡n cÃ³
                    messages=[{"role": m["role"], "content": m["content"]} for m in st.session_state.messages]
                )
                st.session_state.messages.append({"role": "assistant", "content": res.choices[0].message.content})
            st.rerun()
        if c2.button("ğŸ—‘ï¸ Há»§y"):
            st.session_state.voice_draft = None
            st.rerun()

# Chat input máº·c Ä‘á»‹nh
user_input = st.chat_input("Nháº­p tin nháº¯n...")
if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    # Logic tÆ°Æ¡ng tá»± cho chat input (AI hoáº·c áº¢nh)
    st.rerun()
