import streamlit as st
from openai import OpenAI
import google.generativeai as genai

# --- 1. C·∫§U H√åNH H·ªÜ TH·ªêNG & GIAO DI·ªÜN ---
st.set_page_config(page_title="Nexus OS V56.0 - Hyper Memory", layout="wide")

# L·∫•y Keys t·ª´ Secrets
GROQ_KEYS = st.secrets.get("GROQ_KEYS", [])
GEMINI_KEY = st.secrets.get("GEMINI_KEY", "")

# Kh·ªüi t·∫°o Session State
if 'chat_log' not in st.session_state: st.session_state.chat_log = []
if 'bg' not in st.session_state: st.session_state.bg = "https://images.unsplash.com/photo-1614850523296-d8c1af93d400?q=80&w=2070"
if 'current_model' not in st.session_state: st.session_state.current_model = "Auto-Sync"

# CSS T∆∞∆°ng ph·∫£n cao - Ch·ªëng m·ªèi m·∫Øt v√† nh√¨n r√µ ch·ªØ
st.markdown(f"""
    <style>
    .stApp {{
        background: linear-gradient(rgba(0,0,0,0.8), rgba(0,0,0,0.8)), url("{st.session_state.bg}");
        background-size: cover;
    }}
    /* Khung chat t·ªëi ƒë·∫∑c ƒë·ªÉ ch·ªØ tr·∫Øng n·ªïi b·∫≠t */
    .stChatMessage {{
        background: rgba(10, 15, 25, 0.98) !important;
        border: 1px solid #00d2ff;
        color: #ffffff !important;
        border-radius: 15px !important;
    }}
    .stMarkdown p {{ color: #ffffff !important; font-size: 1.1rem; }}
    /* Sidebar chuy√™n nghi·ªáp */
    [data-testid="stSidebar"] {{
        background: rgba(5, 10, 20, 0.95) !important;
        border-right: 2px solid #00d2ff;
    }}
    </style>
    """, unsafe_allow_html=True)

# --- 2. L√ïI QU·∫¢N L√ù B·ªò NH·ªö & PH·∫¢N H·ªíI ---
def get_ai_response(user_input, model_mode):
    # Chu·∫©n b·ªã l·ªãch s·ª≠ h·ªôi tho·∫°i (Tr√≠ nh·ªõ d√†i h·∫°n)
    # L·∫•y 10 c√¢u g·∫ßn nh·∫•t ƒë·ªÉ AI kh√¥ng b·ªã qu√° t·∫£i nh∆∞ng v·∫´n hi·ªÉu b·ªëi c·∫£nh
    history = []
    for m in st.session_state.chat_log[-10:]:
        history.append({"role": m["role"], "content": m["content"]})
    history.append({"role": "user", "content": user_input})

    # DANH S√ÅCH KEY ƒê·ªÇ TH·ª¨
    target_keys = []
    if model_mode == "Auto-Sync":
        target_keys = GROQ_KEYS
    elif "Groq" in model_mode:
        idx = int(model_mode.split(" ")[-1]) - 1
        target_keys = [GROQ_KEYS[idx]]
    
    # 1. TH·ª¨ V·ªöI GROQ
    for i, key in enumerate(target_keys):
        try:
            client = OpenAI(api_key=key, base_url="https://api.groq.com/openai/v1")
            return client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=history, # G·ª≠i to√†n b·ªô l·ªãch s·ª≠ thay v√¨ ch·ªâ 1 c√¢u
                stream=True
            ), f"Groq {i+1 if model_mode == 'Auto-Sync' else model_mode}"
        except Exception:
            if model_mode != "Auto-Sync": break # N·∫øu ch·ªçn Manual m√† l·ªói th√¨ d·ª´ng lu√¥n
            continue

    # 2. D·ª∞ PH√íNG GEMINI (N·∫øu Auto ho·∫∑c Manual Gemini ƒë∆∞·ª£c ch·ªçn)
    if model_mode == "Auto-Sync" or model_mode == "Gemini":
        try:
            genai.configure(api_key=GEMINI_KEY)
            model = genai.GenerativeModel('gemini-1.5-flash')
            # Chuy·ªÉn ƒë·ªïi format history sang format c·ªßa Gemini
            gemini_history = []
            for m in history[:-1]:
                role = "user" if m["role"] == "user" else "model"
                gemini_history.append({"role": role, "parts": [m["content"]]})
            
            chat = model.start_chat(history=gemini_history)
            return chat.send_message(user_input, stream=True), "Gemini Flash"
        except Exception as e:
            return None, str(e)
    
    return None, "L·ªói k·∫øt n·ªëi API"

# --- 3. GIAO DI·ªÜN ƒêI·ªÄU KHI·ªÇN ---
def main():
    with st.sidebar:
        st.title("üí† NEXUS CORE V56")
        
        # B·ªò CH·ªåN CHATBOT REAL-TIME
        st.subheader("ü§ñ C·∫•u h√¨nh AI")
        options = ["Auto-Sync"] + [f"Groq {i+1}" for i in range(len(GROQ_KEYS))] + ["Gemini"]
        st.session_state.current_model = st.selectbox(
            "Ch·ªçn lu·ªìng x·ª≠ l√Ω:", 
            options, 
            index=options.index(st.session_state.current_model)
        )
        
        st.divider()
        st.write(f"üß† Tr√≠ nh·ªõ: **{len(st.session_state.chat_log)} tin nh·∫Øn**")
        if st.button("üóëÔ∏è X√≥a s·∫°ch b·ªô nh·ªõ"):
            st.session_state.chat_log = []
            st.rerun()

    st.title("ü§ñ Neural Terminal")
    st.caption(f"ƒêang s·ª≠ d·ª•ng ch·∫ø ƒë·ªô: **{st.session_state.current_model}** | Ch·ªØ ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u ƒë·ªô t∆∞∆°ng ph·∫£n.")

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for msg in st.session_state.chat_log:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # X·ª≠ l√Ω nh·∫≠p li·ªáu
    if p := st.chat_input("Nh·∫≠p tin nh·∫Øn ƒë·ªÉ ti·∫øp t·ª•c cu·ªôc tr√≤ chuy·ªán..."):
        st.session_state.chat_log.append({"role": "user", "content": p})
        with st.chat_message("user"): st.markdown(p)

        with st.chat_message("assistant"):
            res, source = get_ai_response(p, st.session_state.current_model)
            
            if res:
                box = st.empty(); full = ""
                for chunk in res:
                    # Ki·ªÉm tra xem l√† Groq (OpenAI style) hay Gemini
                    if "Groq" in source:
                        content = chunk.choices[0].delta.content or ""
                    else:
                        content = chunk.text
                    
                    full += content
                    box.markdown(full + "‚ñå")
                
                box.markdown(full)
                st.caption(f"‚ö° Ph·∫£n h·ªìi qua: {source}")
                st.session_state.chat_log.append({"role": "assistant", "content": full})
            else:
                st.error(f"‚ö†Ô∏è Model {st.session_state.current_model} ƒëang b·∫≠n ho·∫∑c sai c·∫•u h√¨nh.")

if __name__ == "__main__":
    main()
