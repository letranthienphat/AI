import streamlit as st
import google.generativeai as genai
from openai import OpenAI
import qrcode
from io import BytesIO
from gtts import gTTS
from streamlit_mic_recorder import mic_recorder

# --- 1. Cáº¤U HÃŒNH ---
st.set_page_config(page_title="AI Live Hub", layout="wide", page_icon="ğŸ™ï¸")

GROQ_API_KEY = st.secrets.get("GROQ_API_KEY", "")

if "messages" not in st.session_state:
    st.session_state.messages = []
if "auto_read" not in st.session_state:
    st.session_state.auto_read = False

# --- HÃ€M Xá»¬ LÃ Ã‚M THANH ---
def text_to_speech(text):
    tts = gTTS(text=text, lang='vi')
    fp = BytesIO()
    tts.write_to_fp(fp)
    return fp.getvalue()

# --- HÃ€M Táº O QR ---
def generate_qr_codes(text, chunk_size=1000):
    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
    qr_images = []
    for i, chunk in enumerate(chunks):
        qr = qrcode.make(f"Part {i+1}/{len(chunks)}:\n{chunk}")
        buf = BytesIO()
        qr.save(buf, format="PNG")
        qr_images.append(buf.getvalue())
    return qr_images

# --- 2. THANH BÃŠN (SIDEBAR) ---
with st.sidebar:
    st.title("ğŸ™ï¸ AI Live Settings")
    live_mode = st.checkbox("Cháº¿ Ä‘á»™ Live (Tá»± Ä‘á»™ng má»Ÿ Mic)", value=False)
    
    st.divider()
    if st.button("ğŸ“„ Táº¡o mÃ£ QR Lá»‹ch sá»­"):
        full_text = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages])
        st.session_state.qr_results = generate_qr_codes(full_text)

    if st.button("ğŸ—‘ï¸ XÃ³a há»™i thoáº¡i"):
        st.session_state.messages = []
        st.rerun()

# --- 3. GIAO DIá»†N CHAT ---
st.title("ğŸ¤– AI Voice & Live Assistant")

# Hiá»ƒn thá»‹ QR (Náº¿u cÃ³)
if "qr_results" in st.session_state:
    cols = st.columns(len(st.session_state.qr_results))
    for idx, qr in enumerate(st.session_state.qr_results):
        with cols[idx]:
            st.image(qr, caption=f"Pháº§n {idx+1}")
            # TrÃ¬nh duyá»‡t sáº½ tá»± há»i vá»‹ trÃ­ lÆ°u khi nháº¥n nÃºt nÃ y
            st.download_button("ğŸ’¾ LÆ°u vá» mÃ¡y", data=qr, file_name=f"chat_qr_part_{idx+1}.png", mime="image/png")

# Hiá»ƒn thá»‹ há»™i thoáº¡i
for i, m in enumerate(st.session_state.messages):
    with st.chat_message(m["role"]):
        st.markdown(m["content"])
        # NÃºt Ä‘á»c láº¡i thá»§ cÃ´ng
        if m["role"] == "assistant":
            if st.button(f"ğŸ”Š Äá»c", key=f"tts_{i}"):
                audio_data = text_to_speech(m["content"])
                st.audio(audio_data, format="audio/mp3", autoplay=True)

# --- 4. NHáº¬P LIá»†U (GIá»ŒNG NÃ“I & CHá»®) ---
st.write("---")
c1, c2 = st.columns([9, 1])

with c2:
    # NÃºt Micro Ä‘á»ƒ nÃ³i (STT)
    audio_input = mic_recorder(start_prompt="ğŸ™ï¸", stop_prompt="ğŸ›‘", key='mic')

with c1:
    text_input = st.chat_input("Nháº­p tin nháº¯n hoáº·c nháº¥n Mic...")

# Xá»­ lÃ½ Logic gá»­i tin
input_data = None
used_voice = False

if audio_input:
    # á» phiÃªn báº£n nÃ y, mic_recorder tráº£ vá» audio. Trong thá»±c táº¿ cáº§n gá»i API STT (nhÆ° OpenAI Whisper) 
    # Ä‘á»ƒ chuyá»ƒn audio thÃ nh text. Äá»ƒ demo, ta giáº£ Ä‘á»‹nh text tá»« audio.
    # LÆ¯U Ã: mic_recorder cáº§n káº¿t ná»‘i API Whisper Ä‘á»ƒ dá»‹ch chÃ­nh xÃ¡c.
    input_data = "TÃ­nh nÄƒng nÃ³i Ä‘ang Ä‘Æ°á»£c káº¿t ná»‘i..." # Placeholder
    used_voice = True
elif text_input:
    input_data = text_input

if input_data:
    st.session_state.messages.append({"role": "user", "content": input_data})
    
    with st.chat_message("assistant"):
        res_area = st.empty()
        full_res = ""
        client = OpenAI(api_key=GROQ_API_KEY, base_url="https://api.groq.com/openai/v1")
        stream = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[{"role": m["role"], "content": m["content"]} for m in st.session_state.messages],
            stream=True
        )
        for chunk in stream:
            if chunk.choices[0].delta.content:
                full_res += chunk.choices[0].delta.content
                res_area.markdown(full_res + "â–Œ")
        res_area.markdown(full_res)
        st.session_state.messages.append({"role": "assistant", "content": full_res})
        
        # Tá»± Ä‘á»™ng Ä‘á»c náº¿u dÃ¹ng giá»ng nÃ³i hoáº·c báº­t Live Mode
        if used_voice or live_mode:
            audio_res = text_to_speech(full_res)
            st.audio(audio_res, format="audio/mp3", autoplay=True)
            
        if live_mode:
            st.info("Äang láº¯ng nghe... (Live Mode)")
            # Live mode sáº½ Ä‘á»£i báº¡n nháº¥n Mic tiáº¿p theo
