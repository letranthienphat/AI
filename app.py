import streamlit as st
from openai import OpenAI
import google.generativeai as genai
import time

# --- 1. C·∫§U H√åNH H·ªÜ TH·ªêNG C·ª∞C M·∫†NH ---
st.set_page_config(page_title="NEXUS ULTIMATE", layout="wide", page_icon="‚ò¢Ô∏è")

# L·∫•y API Keys
GROQ_KEYS = st.secrets.get("GROQ_KEYS", [])
GEMINI_KEY = st.secrets.get("GEMINI_KEY", "")

# Kh·ªüi t·∫°o b·ªô nh·ªõ (SESSION STATE)
if 'chat_log' not in st.session_state: st.session_state.chat_log = []
if 'selected_model' not in st.session_state: st.session_state.selected_model = "Groq 1 (Llama 3.3)"

# --- 2. GIAO DI·ªÜN CYBERPUNK (CSS N√ÇNG CAO) ---
st.markdown("""
    <style>
    /* H√¨nh n·ªÅn ƒë·ªông d·∫°ng l∆∞·ªõi */
    .stApp {
        background-color: #000000;
        background-image: 
            linear-gradient(rgba(0, 255, 255, 0.03) 1px, transparent 1px),
            linear-gradient(90deg, rgba(0, 255, 255, 0.03) 1px, transparent 1px);
        background-size: 30px 30px;
    }
    
    /* Hi·ªáu ·ª©ng ch·ªØ ph√°t s√°ng */
    h1 {
        color: #00e5ff !important;
        text-shadow: 0 0 10px #00e5ff, 0 0 20px #00e5ff;
        font-family: 'Courier New', monospace;
        font-weight: bold;
    }
    
    /* Khung chat ng∆∞·ªùi d√πng */
    .stChatMessage[data-testid="stChatMessage"]:nth-child(odd) {
        background: rgba(0, 229, 255, 0.1) !important;
        border-right: 3px solid #00e5ff;
        border-radius: 10px 0 0 10px;
        color: white !important;
    }

    /* Khung chat AI */
    .stChatMessage[data-testid="stChatMessage"]:nth-child(even) {
        background: rgba(255, 0, 85, 0.1) !important;
        border-left: 3px solid #ff0055;
        border-radius: 0 10px 10px 0;
        color: white !important;
    }

    /* Sidebar k√≠nh c∆∞·ªùng l·ª±c */
    [data-testid="stSidebar"] {
        background: rgba(10, 10, 10, 0.95) !important;
        border-right: 1px solid #333;
    }
    
    /* N√∫t b·∫•m Neon */
    .stButton button {
        border: 1px solid #00e5ff;
        background: transparent;
        color: #00e5ff;
        transition: all 0.3s;
    }
    .stButton button:hover {
        background: #00e5ff;
        color: black;
        box-shadow: 0 0 15px #00e5ff;
    }
    </style>
""", unsafe_allow_html=True)

# --- 3. B·ªò N√ÉO X·ª¨ L√ù (THE BRAIN) ---
def get_groq_response(messages, key_index):
    """G·ªçi Groq v√† g·ª≠i TO√ÄN B·ªò l·ªãch s·ª≠"""
    try:
        client = OpenAI(api_key=GROQ_KEYS[key_index], base_url="https://api.groq.com/openai/v1")
        return client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=messages, # G·ª≠i to√†n b·ªô list messages ƒë·ªÉ AI nh·ªõ
            stream=True
        )
    except Exception as e:
        return None

def get_gemini_response(messages):
    """G·ªçi Gemini v√† g·ª≠i TO√ÄN B·ªò l·ªãch s·ª≠"""
    try:
        genai.configure(api_key=GEMINI_KEY)
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        # Chuy·ªÉn ƒë·ªïi ƒë·ªãnh d·∫°ng l·ªãch s·ª≠ cho Gemini
        gemini_hist = []
        for msg in messages[:-1]: # L·∫•y qu√° kh·ª©
            role = "user" if msg["role"] == "user" else "model"
            gemini_hist.append({"role": role, "parts": [msg["content"]]})
        
        chat = model.start_chat(history=gemini_hist)
        return chat.send_message(messages[-1]["content"], stream=True)
    except Exception as e:
        return None

def ai_engine(user_input, preferred_model):
    # 1. C·∫≠p nh·∫≠t v√†o b·ªô nh·ªõ t·∫°m ƒë·ªÉ g·ª≠i ƒëi (nh∆∞ng ch∆∞a l∆∞u v√†o session state v·ªôi)
    temp_memory = st.session_state.chat_log.copy()
    temp_memory.append({"role": "user", "content": user_input})

    stream = None
    used_model = ""

    # 2. X·ª≠ l√Ω theo l·ª±a ch·ªçn c·ªßa ng∆∞·ªùi d√πng
    if "Groq" in preferred_model:
        idx = int(preferred_model.split(" ")[1]) - 1 # L·∫•y s·ªë 1, 2, 3...
        stream = get_groq_response(temp_memory, idx)
        used_model = preferred_model
        
        # N·∫øu Groq n√†y l·ªói, T·ª∞ ƒê·ªòNG nh·∫£y sang Groq kh√°c (Fail-over)
        if not stream:
            for i in range(len(GROQ_KEYS)):
                if i != idx:
                    stream = get_groq_response(temp_memory, i)
                    if stream: 
                        used_model = f"Groq {i+1} (Auto-Switch)"
                        break
    
    # 3. N·∫øu v·∫´n ch∆∞a c√≥ (ho·∫∑c ch·ªçn Gemini), d√πng Gemini
    if not stream or "Gemini" in preferred_model:
        stream = get_gemini_response(temp_memory)
        used_model = "Gemini" if stream else "SYSTEM FAILURE"

    return stream, used_model

# --- 4. GIAO DI·ªÜN CH√çNH ---
def main():
    # SIDEBAR: Trung t√¢m ch·ªâ huy
    with st.sidebar:
        st.title("üéõÔ∏è SYSTEM CONTROL")
        st.markdown("---")
        
        # Model Selector
        model_options = [f"Groq {i+1} (Llama 3.3)" for i in range(len(GROQ_KEYS))] + ["Gemini (Google)"]
        st.session_state.selected_model = st.radio(
            "üì° Ch·ªçn K√™nh K·∫øt N·ªëi:", 
            model_options,
            index=model_options.index(st.session_state.selected_model) if st.session_state.selected_model in model_options else 0
        )
        
        st.markdown("---")
        st.write(f"üß† B·ªô nh·ªõ ƒë·ªám: **{len(st.session_state.chat_log)} d√≤ng**")
        if st.button("üî¥ KH·ªûI ƒê·ªòNG L·∫†I (RESET)"):
            st.session_state.chat_log = []
            st.rerun()

    # MAIN SCREEN
    st.title("NEXUS /// ULTIMATE")
    
    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat (Memory Playback)
    for msg in st.session_state.chat_log:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # √î nh·∫≠p li·ªáu
    if prompt := st.chat_input("Nh·∫≠p l·ªánh k√≠ch ho·∫°t..."):
        # Hi·ªÉn th·ªã tin nh·∫Øn ng∆∞·ªùi d√πng ngay l·∫≠p t·ª©c
        st.session_state.chat_log.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # X·ª≠ l√Ω AI
        with st.chat_message("assistant"):
            status_box = st.empty()
            status_box.caption("üîÑ ƒêang k·∫øt n·ªëi neural network...")
            
            stream, source = ai_engine(prompt, st.session_state.selected_model)
            
            if stream:
                status_box.caption(f"‚ö° ƒê√£ k·∫øt n·ªëi: **{source}**")
                response_container = st.empty()
                full_response = ""
                
                # Streaming effect
                for chunk in stream:
                    content = ""
                    if "Groq" in source:
                        content = chunk.choices[0].delta.content or ""
                    else:
                        content = chunk.text
                    
                    full_response += content
                    response_container.markdown(full_response + "‚ñà") # Con tr·ªè nh·∫•p nh√°y
                
                response_container.markdown(full_response)
                # L∆ØU V√ÄO B·ªò NH·ªö Vƒ®NH C·ª¨U
                st.session_state.chat_log.append({"role": "assistant", "content": full_response})
            else:
                st.error("‚ùå M·∫§T K·∫æT N·ªêI TO√ÄN B·ªò SERVER!")

if __name__ == "__main__":
    main()
